{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731ecd38",
   "metadata": {},
   "source": [
    "# **Part 01: Apprentissage avec des Benchmarks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c67d5",
   "metadata": {},
   "source": [
    "## **1. Préparation et exploration des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f787763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abalone8 (4177, 10) (4177,)\n",
      "abalone17 (4177, 10) (4177,)\n",
      "abalone20 (4177, 10) (4177,)\n",
      "autompg (392, 7) (392,)\n",
      "australian (690, 14) (690,)\n",
      "balance (625, 4) (625,)\n",
      "bankmarketing (45211, 51) (45211,)\n",
      "bupa (345, 6) (345,)\n",
      "german (1000, 24) (1000,)\n",
      "glass (214, 9) (214,)\n",
      "hayes (132, 4) (132,)\n",
      "heart (270, 13) (270,)\n",
      "iono (351, 34) (351,)\n",
      "libras (360, 90) (360,)\n",
      "newthyroid (215, 5) (215,)\n",
      "pageblocks (5473, 10) (5473,)\n",
      "pima (768, 8) (768,)\n",
      "satimage (6435, 36) (6435,)\n",
      "segmentation (2310, 19) (2310,)\n",
      "sonar (208, 60) (208,)\n",
      "spambase (4597, 57) (4597,)\n",
      "splice (3175, 60) (3175,)\n",
      "vehicle (846, 18) (846,)\n",
      "wdbc (569, 30) (569,)\n",
      "wine (178, 13) (178,)\n",
      "wine4 (1599, 11) (1599,)\n",
      "yeast3 (1484, 8) (1484,)\n",
      "yeast6 (1484, 8) (1484,)\n"
     ]
    }
   ],
   "source": [
    "# Téléchargement des données qui sont dans le répertoire datasets \n",
    "from prepdata import data_recovery\n",
    "\n",
    "datasets = [\n",
    "    \"abalone8\", \"abalone17\", \"abalone20\",\n",
    "    \"autompg\", \"australian\", \"balance\", \"bankmarketing\",\n",
    "    \"bupa\", \"german\", \"glass\", \"hayes\", \"heart\", \"iono\",\n",
    "    \"libras\", \"newthyroid\", \"pageblocks\", \"pima\",\n",
    "    \"satimage\", \"segmentation\", \"sonar\", \"spambase\",\n",
    "    \"splice\", \"vehicle\", \"wdbc\", \"wine\", \"wine4\",\n",
    "    \"yeast3\", \"yeast6\",\n",
    "]\n",
    "\n",
    "for name in datasets:\n",
    "    X, y = data_recovery(name)\n",
    "    print(name, X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b3255ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          dataset  n_samples  n_features  ratio_positive  ratio_negative  \\\n",
      "0        abalone8       4177          10        0.135983        0.864017   \n",
      "1       abalone17       4177          10        0.013886        0.986114   \n",
      "2       abalone20       4177          10        0.006225        0.993775   \n",
      "3         autompg        392           7        0.375000        0.625000   \n",
      "4      australian        690          14        0.444928        0.555072   \n",
      "5         balance        625           4        0.460800        0.539200   \n",
      "6   bankmarketing      45211          51        0.116985        0.883015   \n",
      "7            bupa        345           6        0.420290        0.579710   \n",
      "8          german       1000          24        0.300000        0.700000   \n",
      "9           glass        214           9        0.327103        0.672897   \n",
      "10          hayes        132           4        0.227273        0.772727   \n",
      "11          heart        270          13        0.444444        0.555556   \n",
      "12           iono        351          34        0.358974        0.641026   \n",
      "13         libras        360          90        0.066667        0.933333   \n",
      "14     newthyroid        215           5        0.302326        0.697674   \n",
      "15     pageblocks       5473          10        0.102320        0.897680   \n",
      "16           pima        768           8        0.348958        0.651042   \n",
      "17       satimage       6435          36        0.097280        0.902720   \n",
      "18   segmentation       2310          19        0.142857        0.857143   \n",
      "19          sonar        208          60        0.466346        0.533654   \n",
      "20       spambase       4597          57        0.394170        0.605830   \n",
      "21         splice       3175          60        0.480945        0.519055   \n",
      "22        vehicle        846          18        0.235225        0.764775   \n",
      "23           wdbc        569          30        0.372583        0.627417   \n",
      "24           wine        178          13        0.331461        0.668539   \n",
      "25          wine4       1599          11        0.033146        0.966854   \n",
      "26         yeast3       1484           8        0.109838        0.890162   \n",
      "27         yeast6       1484           8        0.023585        0.976415   \n",
      "\n",
      "    minority_ratio  imbalanced  has_missing  n_missing           problem_type  \n",
      "0         0.135983        True        False          0  binary_classification  \n",
      "1         0.013886        True        False          0  binary_classification  \n",
      "2         0.006225        True        False          0  binary_classification  \n",
      "3         0.375000       False        False          0  binary_classification  \n",
      "4         0.444928       False        False          0  binary_classification  \n",
      "5         0.460800       False        False          0  binary_classification  \n",
      "6         0.116985        True        False          0  binary_classification  \n",
      "7         0.420290       False        False          0  binary_classification  \n",
      "8         0.300000       False        False          0  binary_classification  \n",
      "9         0.327103       False        False          0  binary_classification  \n",
      "10        0.227273        True        False          0  binary_classification  \n",
      "11        0.444444       False        False          0  binary_classification  \n",
      "12        0.358974       False        False          0  binary_classification  \n",
      "13        0.066667        True        False          0  binary_classification  \n",
      "14        0.302326       False        False          0  binary_classification  \n",
      "15        0.102320        True        False          0  binary_classification  \n",
      "16        0.348958       False        False          0  binary_classification  \n",
      "17        0.097280        True        False          0  binary_classification  \n",
      "18        0.142857        True        False          0  binary_classification  \n",
      "19        0.466346       False        False          0  binary_classification  \n",
      "20        0.394170       False        False          0  binary_classification  \n",
      "21        0.480945       False        False          0  binary_classification  \n",
      "22        0.235225        True        False          0  binary_classification  \n",
      "23        0.372583       False        False          0  binary_classification  \n",
      "24        0.331461       False        False          0  binary_classification  \n",
      "25        0.033146        True        False          0  binary_classification  \n",
      "26        0.109838        True        False          0  binary_classification  \n",
      "27        0.023585        True        False          0  binary_classification  \n",
      "\n",
      "Datasets déséquilibrés (< 30% de la classe minoritaire) :\n",
      "['abalone8', 'abalone17', 'abalone20', 'bankmarketing', 'hayes', 'libras', 'pageblocks', 'satimage', 'segmentation', 'vehicle', 'wine4', 'yeast3', 'yeast6']\n",
      "Nombre de datasets déséquilibrés : 13\n",
      "Nombre de datasets avec des valeurs manquantes : 0\n"
     ]
    }
   ],
   "source": [
    "# Construction d'une table récapitulative des informations sur les datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prepdata import data_recovery\n",
    "\n",
    "datasets = [\n",
    "    \"abalone8\", \"abalone17\", \"abalone20\",\n",
    "    \"autompg\", \"australian\", \"balance\", \"bankmarketing\",\n",
    "    \"bupa\", \"german\", \"glass\", \"hayes\", \"heart\", \"iono\",\n",
    "    \"libras\", \"newthyroid\", \"pageblocks\", \"pima\",\n",
    "    \"satimage\", \"segmentation\", \"sonar\", \"spambase\",\n",
    "    \"splice\", \"vehicle\", \"wdbc\", \"wine\", \"wine4\",\n",
    "    \"yeast3\", \"yeast6\",\n",
    "]\n",
    "\n",
    "info_rows = []\n",
    "imbalanced_datasets = []   # pour stocker les jeux déséquilibrés\n",
    "\n",
    "threshold = 0.3            # classe minoritaire < 30% => déséquilibré\n",
    "\n",
    "for name in datasets:\n",
    "    X, y = data_recovery(name)\n",
    "\n",
    "    n, d = X.shape\n",
    "    ratio_positive = np.mean(y == 1)\n",
    "    ratio_negative = np.mean(y == 0)\n",
    "\n",
    "    # minoritaire = min(ratio_pos, ratio_neg)\n",
    "    minority_ratio = min(ratio_positive, ratio_negative)\n",
    "\n",
    "\n",
    "    # détection valeurs manquantes\n",
    "    n_missing = np.isnan(X).sum()\n",
    "    has_missing = n_missing > 0\n",
    "\n",
    "    info_rows.append({\n",
    "        \"dataset\": name,\n",
    "        \"n_samples\": n,\n",
    "        \"n_features\": d,\n",
    "        \"ratio_positive\": ratio_positive,\n",
    "        \"ratio_negative\": ratio_negative,\n",
    "        \"minority_ratio\": minority_ratio,\n",
    "        \"imbalanced\": minority_ratio < threshold,  \n",
    "        \"has_missing\": has_missing,\n",
    "        \"n_missing\": int(n_missing),\n",
    "        \"problem_type\": \"binary_classification\",\n",
    "    })\n",
    "\n",
    "    # si dataset déséquilibré, on l'ajoute à une liste spéciale\n",
    "    if minority_ratio < threshold:\n",
    "        imbalanced_datasets.append(name)\n",
    "\n",
    "info_df = pd.DataFrame(info_rows)\n",
    "print(info_df)\n",
    "info_df.to_csv(\"dataset_info_summary.csv\", index=False)\n",
    "\n",
    "# Affichage des datasets déséquilibrés\n",
    "print(\"\\nDatasets déséquilibrés (< 30% de la classe minoritaire) :\")\n",
    "print(imbalanced_datasets)\n",
    "print(\"Nombre de datasets déséquilibrés :\", len(imbalanced_datasets))\n",
    "\n",
    "#Nombre de datasets qui contiennent des valeurs manquantes\n",
    "n_datasets_with_missing = info_df[\"has_missing\"].sum()\n",
    "print(\"Nombre de datasets avec des valeurs manquantes :\", n_datasets_with_missing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7af81",
   "metadata": {},
   "source": [
    "## **2. Protocole commun**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d6746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le but est de définir une fonction générique avec un split train/test, les métriques et la mesure du temps d’apprentissage. On va l'utiliser par la suite pour tous les datasets.\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Imortation des métriques qu'on va utiliser\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "def evaluate_model(model, X, y, test_size=0.3, random_state=0):\n",
    "    \"\"\"\n",
    "    model : estimateur sklearn déjà configuré \n",
    "    X, y : données complètes\n",
    "    Retourne un dict avec accuracy, f1, auc, temps d'apprentissage.\n",
    "    \"\"\"\n",
    "    # découpe stratifiée\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # temps d'apprentissage\n",
    "    t0 = perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    t1 = perf_counter()\n",
    "    train_time = t1 - t0\n",
    "\n",
    "    # prédictions de classes\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)   # F1 pour la classe positive \n",
    "\n",
    "    # AUC: il faut des scores/probas, pas des classes\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_scores = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            # certains modèles (SVM linéaire, etc.) ont decision_function\n",
    "            y_scores = model.decision_function(X_test)\n",
    "        auc = roc_auc_score(y_test, y_scores)   # ROC AUC binaire \n",
    "    except Exception:\n",
    "        auc = np.nan   # si vraiment pas possible\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"train_time\": train_time,\n",
    "        \"n_train\": len(y_train),\n",
    "        \"n_test\": len(y_test),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bcb913",
   "metadata": {},
   "source": [
    "## **3. Approches non paramétriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sur cette partie, on va se concentrer sur le KNN avec ses différentes variantes.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Définition des variantes KNN\n",
    "\n",
    "\n",
    "# 1) KNN de base : k=5, distance euclidienne, poids uniformes \n",
    "knn_base = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights=\"uniform\",\n",
    "    metric=\"minkowski\",\n",
    "    p=2,\n",
    ")\n",
    "\n",
    "# 2) KNN pondéré par la distance : les voisins proches comptent plus \n",
    "knn_distance = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights=\"distance\",\n",
    "    metric=\"minkowski\",\n",
    "    p=2,\n",
    ")\n",
    "\n",
    "# 3) KNN avec normalisation des features : StandardScaler + KNN \n",
    "knn_scaled = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier(\n",
    "        n_neighbors=5,\n",
    "        weights=\"uniform\",\n",
    "        metric=\"minkowski\",\n",
    "        p=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 4) KNN avec k optimisé par validation croisée (GridSearchCV)\n",
    "#    On cherche le meilleur n_neighbors dans {1,3,5,7,11}\n",
    "def make_knn_cv():\n",
    "    \"\"\"\n",
    "    Crée un GridSearchCV neuf pour KNN :\n",
    "    - pipeline StandardScaler + KNN\n",
    "    - recherche de k via validation croisée\n",
    "    \"\"\"\n",
    "    base_pipe = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        KNeighborsClassifier(weights=\"uniform\"),\n",
    "    )\n",
    "    param_grid = {\n",
    "        \"kneighborsclassifier__n_neighbors\": [1, 3, 5, 7, 11],\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_pipe,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"f1\",   # on optimise le F1-score \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    return grid\n",
    "\n",
    "# 5) KNN avec SMOTE + normalisation (pour données déséquilibrées), méthode qu'on utilisera seulement si le dataset est déséquilibré, d'ou l'utilité de la colonne qu'on a rajouté dans le tableau récapitulatif\n",
    "knn_smote = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"smote\", SMOTE(random_state=0)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"knn\", KNeighborsClassifier(\n",
    "            n_neighbors=5,\n",
    "            weights=\"uniform\",\n",
    "            metric=\"minkowski\",\n",
    "            p=2,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dictionnaire pour le tableau benchmark\n",
    "models_knn = {\n",
    "    \"knn_base\": knn_base,\n",
    "    \"knn_distance\": knn_distance,\n",
    "    \"knn_scaled\": knn_scaled,\n",
    "    \"knn_cv\": make_knn_cv,   \n",
    "    \"knn_smote\": knn_smote,  \n",
    "}\n",
    "\n",
    "\n",
    "# Evaluation de toutes les variantes KNN\n",
    "\n",
    "results_rows = []\n",
    "\n",
    "for ds in datasets:   \n",
    "    # vérifier si le dataset est déséquilibré\n",
    "    row_info = info_df[info_df[\"dataset\"] == ds].iloc[0]\n",
    "    is_imbalanced = bool(row_info[\"imbalanced\"])\n",
    "\n",
    "    # charger X, y\n",
    "    X, y = data_recovery(ds)\n",
    "\n",
    "    for model_name, model_or_factory in models_knn.items():\n",
    "        # on skippe knn_smote si le dataset est équilibré\n",
    "        if (model_name == \"knn_smote\") and (not is_imbalanced):\n",
    "            continue\n",
    "\n",
    "    \n",
    "        if callable(model_or_factory):\n",
    "            model = model_or_factory()\n",
    "        else:\n",
    "            model = model_or_factory\n",
    "\n",
    "        res = evaluate_model(model, X, y)\n",
    "        res.update({\n",
    "            \"dataset\": ds,\n",
    "            \"model_family\": \"knn\",\n",
    "            \"model_name\": model_name,\n",
    "            \"imbalanced\": is_imbalanced,\n",
    "        })\n",
    "        results_rows.append(res)\n",
    "\n",
    "results_knn_df = pd.DataFrame(results_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd98b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name     knn_base    knn_cv  knn_distance  knn_scaled  knn_smote\n",
      "dataset                                                               \n",
      "abalone17      0.984051  0.977671      0.984051    0.985646   0.901914\n",
      "abalone20      0.993620  0.985646      0.993620    0.993620   0.938596\n",
      "abalone8       0.834928  0.806220      0.836523    0.830144   0.671451\n",
      "australian     0.671498  0.859903      0.657005    0.835749        NaN\n",
      "autompg        0.737288  0.762712      0.788136    0.754237        NaN\n",
      "balance        0.904255  0.930851      0.904255    0.888298        NaN\n",
      "bankmarketing  0.882409  0.875184      0.881303    0.892215   0.854615\n",
      "bupa           0.653846  0.596154      0.653846    0.634615        NaN\n",
      "german         0.676667  0.676667      0.683333    0.700000        NaN\n",
      "glass          0.738462  0.830769      0.753846    0.723077        NaN\n",
      "hayes          0.825000  0.875000      0.825000    0.875000   0.850000\n",
      "heart          0.641975  0.839506      0.654321    0.814815        NaN\n",
      "iono           0.867925  0.849057      0.867925    0.858491        NaN\n",
      "libras         0.962963  0.981481      0.962963    0.962963   0.990741\n",
      "newthyroid     0.984615  0.984615      0.969231    0.984615        NaN\n",
      "pageblocks     0.961023  0.958587      0.965895    0.963459   0.953715\n",
      "pima           0.744589  0.770563      0.727273    0.731602        NaN\n",
      "satimage       0.937338  0.938374      0.937856    0.935267   0.887105\n",
      "segmentation   0.965368  0.976912      0.974026    0.971140   0.969697\n",
      "sonar          0.841270  0.920635      0.873016    0.825397        NaN\n",
      "spambase       0.787681  0.897101      0.813768    0.903623        NaN\n",
      "splice         0.739769  0.749213      0.740818    0.749213        NaN\n",
      "vehicle        0.960630  0.944882      0.964567    0.944882   0.913386\n",
      "wdbc           0.935673  0.941520      0.935673    0.941520        NaN\n",
      "wine           0.888889  0.981481      0.888889    1.000000        NaN\n",
      "wine4          0.962500  0.954167      0.960417    0.966667   0.852083\n",
      "yeast3         0.952915  0.955157      0.950673    0.957399   0.928251\n",
      "yeast6         0.986547  0.984305      0.984305    0.984305   0.930493\n"
     ]
    }
   ],
   "source": [
    "# Affichage d'un tableau avec lignes = datasets, colonnes = variantes KNN, valeur = accuracy\n",
    "# on peut faire pareil oui pour f1, auc, train_time \n",
    "acc_table = results_knn_df.pivot_table(\n",
    "    index=\"dataset\",\n",
    "    columns=\"model_name\",\n",
    "    values=\"accuracy\"\n",
    ")\n",
    "print(acc_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6fcb669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name     knn_base    knn_cv  knn_distance  knn_scaled  knn_smote\n",
      "dataset                                                               \n",
      "abalone17      0.000000  0.125000      0.000000    0.000000   0.102190\n",
      "abalone20      0.000000  0.000000      0.000000    0.000000   0.049383\n",
      "abalone8       0.181818  0.278932      0.196078    0.171206   0.339744\n",
      "australian     0.626374  0.828402      0.607735    0.808989        NaN\n",
      "autompg        0.651685  0.695652      0.725275    0.674157        NaN\n",
      "balance        0.898876  0.924855      0.898876    0.881356        NaN\n",
      "bankmarketing  0.356595  0.432070      0.365142    0.406656   0.471597\n",
      "bupa           0.526316  0.522727      0.526316    0.558140        NaN\n",
      "german         0.340136  0.469945      0.362416    0.357143        NaN\n",
      "glass          0.638298  0.755556      0.652174    0.608696        NaN\n",
      "hayes          0.461538  0.666667      0.461538    0.666667   0.666667\n",
      "heart          0.591549  0.816901      0.600000    0.788732        NaN\n",
      "iono           0.781250  0.757576      0.781250    0.761905        NaN\n",
      "libras         0.600000  0.833333      0.600000    0.600000   0.933333\n",
      "newthyroid     0.974359  0.975610      0.950000    0.974359        NaN\n",
      "pageblocks     0.792208  0.791411      0.820513    0.811321   0.805128\n",
      "pima           0.624204  0.653595      0.593548    0.575342        NaN\n",
      "satimage       0.659155  0.679245      0.661017    0.661247   0.616197\n",
      "segmentation   0.876289  0.915789      0.908163    0.893617   0.903226\n",
      "sonar          0.821429  0.915254      0.857143    0.800000        NaN\n",
      "spambase       0.740018  0.865784      0.771961    0.875817        NaN\n",
      "splice         0.773309  0.782133      0.774016    0.782133        NaN\n",
      "vehicle        0.919355  0.883333      0.928000    0.883333   0.840580\n",
      "wdbc           0.913386  0.923077      0.914729    0.923077        NaN\n",
      "wine           0.850000  0.972973      0.850000    1.000000        NaN\n",
      "wine4          0.000000  0.083333      0.000000    0.000000   0.183908\n",
      "yeast3         0.758621  0.772727      0.744186    0.786517   0.724138\n",
      "yeast6         0.700000  0.631579      0.666667    0.631579   0.367347\n"
     ]
    }
   ],
   "source": [
    "# On compare avec F1, vu que l'accuracy n'est pas toujours le meilleur indicateur sur des datasets déséquilibrés\n",
    "f1_table = results_knn_df.pivot_table(\n",
    "    index=\"dataset\",\n",
    "    columns=\"model_name\",\n",
    "    values=\"f1\"\n",
    ")\n",
    "print(f1_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97ad3b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name     knn_base    knn_cv  knn_distance  knn_scaled  knn_smote\n",
      "dataset                                                               \n",
      "abalone17      0.566836  0.553569      0.567550    0.565433   0.702054\n",
      "abalone20      0.489968  0.495987      0.489968    0.547151   0.596810\n",
      "abalone8       0.679764  0.582487      0.676948    0.671769   0.711253\n",
      "australian     0.710208  0.910491      0.700000    0.898015        NaN\n",
      "autompg        0.846898  0.847666      0.862869    0.867322        NaN\n",
      "balance        0.972801  0.983328      0.973939    0.967680        NaN\n",
      "bankmarketing  0.766091  0.671589      0.765568    0.797585   0.793856\n",
      "bupa           0.712689  0.586364      0.717424    0.664583        NaN\n",
      "german         0.611217  0.619841      0.620794    0.677910        NaN\n",
      "glass          0.833333  0.825216      0.849567    0.771104        NaN\n",
      "hayes          0.976703  0.973118      0.978495    0.973118   0.913978\n",
      "heart          0.726235  0.908333      0.720988    0.847531        NaN\n",
      "iono           0.958591  0.806889      0.955882    0.958204        NaN\n",
      "libras         0.998586  0.857143      1.000000    1.000000   1.000000\n",
      "newthyroid     0.995000  0.988889      0.997778    0.999444        NaN\n",
      "pageblocks     0.958699  0.874091      0.957665    0.972794   0.977465\n",
      "pima           0.771605  0.783128      0.775144    0.785350        NaN\n",
      "satimage       0.923466  0.906233      0.926826    0.934556   0.946477\n",
      "segmentation   0.991914  0.936027      0.996174    0.989227   0.991634\n",
      "sonar          0.908722  0.921400      0.937120    0.925456        NaN\n",
      "spambase       0.862961  0.955859      0.888535    0.951062        NaN\n",
      "splice         0.866219  0.869723      0.881781    0.869723        NaN\n",
      "vehicle        0.980155  0.980498      0.983591    0.980498   0.979124\n",
      "wdbc           0.964077  0.976562      0.965537    0.976562        NaN\n",
      "wine           0.958333  0.999228      0.964506    1.000000        NaN\n",
      "wine4          0.472117  0.523707      0.489157    0.737810   0.704203\n",
      "yeast3         0.902971  0.942040      0.902457    0.924408   0.915797\n",
      "yeast6         0.943783  0.892163      0.941170    0.896656   0.883177\n"
     ]
    }
   ],
   "source": [
    "# Pareil pour l'AUC \n",
    "auc_table = results_knn_df.pivot_table(\n",
    "    index=\"dataset\",\n",
    "    columns=\"model_name\",\n",
    "    values=\"auc\"\n",
    ")\n",
    "print(auc_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db44d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            imbalanced  accuracy_knn_base  accuracy_knn_cv  \\\n",
      "dataset                                                      \n",
      "abalone8          True           0.834928         0.806220   \n",
      "abalone17         True           0.984051         0.977671   \n",
      "abalone20         True           0.993620         0.985646   \n",
      "autompg          False           0.737288         0.762712   \n",
      "australian       False           0.671498         0.859903   \n",
      "\n",
      "            accuracy_knn_distance  accuracy_knn_scaled  accuracy_knn_smote  \\\n",
      "dataset                                                                      \n",
      "abalone8                 0.836523             0.830144            0.671451   \n",
      "abalone17                0.984051             0.985646            0.901914   \n",
      "abalone20                0.993620             0.993620            0.938596   \n",
      "autompg                  0.788136             0.754237                 NaN   \n",
      "australian               0.657005             0.835749                 NaN   \n",
      "\n",
      "            f1_knn_base  f1_knn_cv  f1_knn_distance  f1_knn_scaled  \\\n",
      "dataset                                                              \n",
      "abalone8       0.181818   0.278932         0.196078       0.171206   \n",
      "abalone17      0.000000   0.125000         0.000000       0.000000   \n",
      "abalone20      0.000000   0.000000         0.000000       0.000000   \n",
      "autompg        0.651685   0.695652         0.725275       0.674157   \n",
      "australian     0.626374   0.828402         0.607735       0.808989   \n",
      "\n",
      "            f1_knn_smote  auc_knn_base  auc_knn_cv  auc_knn_distance  \\\n",
      "dataset                                                                \n",
      "abalone8        0.339744      0.679764    0.582487          0.676948   \n",
      "abalone17       0.102190      0.566836    0.553569          0.567550   \n",
      "abalone20       0.049383      0.489968    0.495987          0.489968   \n",
      "autompg              NaN      0.846898    0.847666          0.862869   \n",
      "australian           NaN      0.710208    0.910491          0.700000   \n",
      "\n",
      "            auc_knn_scaled  auc_knn_smote  \n",
      "dataset                                    \n",
      "abalone8          0.671769       0.711253  \n",
      "abalone17         0.565433       0.702054  \n",
      "abalone20         0.547151       0.596810  \n",
      "autompg           0.867322            NaN  \n",
      "australian        0.898015            NaN  \n"
     ]
    }
   ],
   "source": [
    "# Une autre forme du tableau \n",
    "# on suppose que info_df contient au moins: dataset, imbalanced\n",
    "knn_with_info = results_knn_df.merge(\n",
    "    info_df[[\"dataset\", \"imbalanced\"]],\n",
    "    on=\"dataset\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# on définit une fonction pratique pour renommer les colonnes après pivot\n",
    "def make_metric_table(df, value_col):\n",
    "    tab = df.pivot_table(\n",
    "        index=\"dataset\",\n",
    "        columns=\"model_name\",\n",
    "        values=value_col\n",
    "    )\n",
    "    # on ajoute le nom de la métrique au niveau des colonnes\n",
    "    tab.columns = [f\"{value_col}_{m}\" for m in tab.columns]\n",
    "    return tab\n",
    "\n",
    "acc_tab = make_metric_table(knn_with_info, \"accuracy\")\n",
    "f1_tab  = make_metric_table(knn_with_info, \"f1\")\n",
    "auc_tab = make_metric_table(knn_with_info, \"auc\")\n",
    "\n",
    "# on récupère aussi la colonne imbalanced (une seule fois par dataset)\n",
    "imb_tab = info_df.set_index(\"dataset\")[[\"imbalanced\"]]\n",
    "\n",
    "# tableau final : imbalanced + toutes les colonnes de métriques\n",
    "knn_full_table = pd.concat(\n",
    "    [imb_tab, acc_tab, f1_tab, auc_tab],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(knn_full_table.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2901d7c",
   "metadata": {},
   "source": [
    "## **4. Approches paramétriques linéaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c40e3f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accuracy        f1       auc  train_time  n_train  n_test    dataset  \\\n",
      "0  0.863636  0.022857  0.755936    0.097377     2923    1254   abalone8   \n",
      "1  0.649123  0.373219  0.757858    0.031555     2923    1254   abalone8   \n",
      "2  0.864434  0.011628  0.751648    0.018558     2923    1254   abalone8   \n",
      "3  0.643541  0.376569  0.757383    0.007366     2923    1254   abalone8   \n",
      "4  0.984848  0.000000  0.883114    0.025880     2923    1254  abalone17   \n",
      "\n",
      "  model_family        model_name  imbalanced  \n",
      "0       linear       logreg_base        True  \n",
      "1       linear   logreg_balanced        True  \n",
      "2       linear      svm_lin_base        True  \n",
      "3       linear  svm_lin_balanced        True  \n",
      "4       linear       logreg_base        True  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Là on va se concentrer sur les modèles linéaires : régression logistique et SVM linéaire, avec deux variantes pour chacun.\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Régression logistique\n",
    "# Variante 1 : LogReg de base (avec standardisation)\n",
    "logreg_base = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Variante 2 : LogReg avec pondération automatique des classes\n",
    "logreg_balanced = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",  # plus de poids à la classe minoritaire \n",
    "    ),\n",
    ")\n",
    "\n",
    "# SVM Linéaire\n",
    "\n",
    "# Variante 1 : LinearSVC de base (avec standardisation)\n",
    "svm_lin_base = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearSVC(\n",
    "        C=1.0,\n",
    "        max_iter=5000,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Variante 2 : LinearSVC avec class_weight balanced\n",
    "svm_lin_balanced = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearSVC(\n",
    "        C=1.0,\n",
    "        class_weight=\"balanced\",  # gère les jeux déséquilibrés \n",
    "        max_iter=5000,\n",
    "    ),\n",
    ")\n",
    "\n",
    "models_linear = {\n",
    "    \"logreg_base\": logreg_base,\n",
    "    \"logreg_balanced\": logreg_balanced,\n",
    "    \"svm_lin_base\": svm_lin_base,\n",
    "    \"svm_lin_balanced\": svm_lin_balanced,\n",
    "}\n",
    "\n",
    "# Evaluation des modèles linéaires\n",
    "results_linear_rows = []\n",
    "\n",
    "for ds in datasets:  \n",
    "    row_info = info_df[info_df[\"dataset\"] == ds].iloc[0]\n",
    "    is_imbalanced = bool(row_info[\"imbalanced\"])\n",
    "\n",
    "    X, y = data_recovery(ds)\n",
    "\n",
    "    for model_name, model in models_linear.items():\n",
    "\n",
    "        # On utilise les variantes \"balanced\" que pour les datasets déséquilibrés\n",
    "        if (\"balanced\" in model_name) and (not is_imbalanced):\n",
    "            continue\n",
    "\n",
    "        res = evaluate_model(model, X, y)\n",
    "        res.update({\n",
    "            \"dataset\": ds,\n",
    "            \"model_family\": \"linear\",\n",
    "            \"model_name\": model_name,\n",
    "            \"imbalanced\": is_imbalanced,\n",
    "        })\n",
    "        results_linear_rows.append(res)\n",
    "\n",
    "results_linear_df = pd.DataFrame(results_linear_rows)\n",
    "print(results_linear_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d4f815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name     logreg_balanced  logreg_base  svm_lin_balanced  svm_lin_base\n",
      "dataset                                                                    \n",
      "abalone17             0.803030     0.984848          0.818182      0.986443\n",
      "abalone20             0.861244     0.993620          0.859649      0.993620\n",
      "abalone8              0.649123     0.863636          0.643541      0.864434\n",
      "australian                 NaN     0.859903               NaN      0.859903\n",
      "autompg                    NaN     0.838983               NaN      0.889831\n",
      "balance                    NaN     0.968085               NaN      0.968085\n",
      "bankmarketing         0.842598     0.901504          0.850044      0.901283\n",
      "bupa                       NaN     0.653846               NaN      0.644231\n",
      "german                     NaN     0.776667               NaN      0.773333\n",
      "glass                      NaN     0.707692               NaN      0.738462\n",
      "hayes                 0.875000     0.875000          0.875000      0.850000\n",
      "heart                      NaN     0.790123               NaN      0.790123\n",
      "iono                       NaN     0.877358               NaN      0.867925\n",
      "libras                0.953704     0.990741          0.972222      0.972222\n",
      "newthyroid                 NaN     0.938462               NaN      0.938462\n",
      "pageblocks            0.917174     0.954324          0.915956      0.953106\n",
      "pima                       NaN     0.774892               NaN      0.774892\n",
      "satimage              0.618850     0.905230          0.606939      0.903677\n",
      "segmentation          0.849928     0.910534          0.849928      0.916306\n",
      "sonar                      NaN     0.841270               NaN      0.777778\n",
      "spambase                   NaN     0.915217               NaN      0.911594\n",
      "splice                     NaN     0.847849               NaN      0.845750\n",
      "vehicle               0.944882     0.940945          0.948819      0.952756\n",
      "wdbc                       NaN     0.959064               NaN      0.941520\n",
      "wine                       NaN     1.000000               NaN      0.981481\n",
      "wine4                 0.700000     0.966667          0.697917      0.966667\n",
      "yeast3                0.905830     0.952915          0.903587      0.952915\n",
      "yeast6                0.885650     0.984305          0.885650      0.979821\n"
     ]
    }
   ],
   "source": [
    "# Construction de la table des résultats avec accuracy\n",
    "acc_table = results_linear_df.pivot_table(\n",
    "    index=\"dataset\",\n",
    "    columns=\"model_name\",\n",
    "    values=\"accuracy\"\n",
    ")\n",
    "print(acc_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "336ae14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name     logreg_balanced  logreg_base  svm_lin_balanced  svm_lin_base\n",
      "dataset                                                                    \n",
      "abalone17             0.101818     0.000000          0.109375      0.000000\n",
      "abalone20             0.064516     0.000000          0.063830      0.000000\n",
      "abalone8              0.373219     0.022857          0.376569      0.011628\n",
      "australian                 NaN     0.841530               NaN      0.846561\n",
      "autompg                    NaN     0.786517               NaN      0.860215\n",
      "balance                    NaN     0.965116               NaN      0.965116\n",
      "bankmarketing         0.552692     0.450658          0.562392      0.420597\n",
      "bupa                       NaN     0.550000               NaN      0.543210\n",
      "german                     NaN     0.567742               NaN      0.552632\n",
      "glass                      NaN     0.486486               NaN      0.540541\n",
      "hayes                 0.782609     0.666667          0.782609      0.666667\n",
      "heart                      NaN     0.753623               NaN      0.753623\n",
      "iono                       NaN     0.826667               NaN      0.820513\n",
      "libras                0.736842     0.923077          0.769231      0.769231\n",
      "newthyroid                 NaN     0.888889               NaN      0.888889\n",
      "pageblocks            0.697778     0.749164          0.686364      0.731707\n",
      "pima                       NaN     0.633803               NaN      0.633803\n",
      "satimage              0.286822     0.061538          0.293953      0.021053\n",
      "segmentation          0.638889     0.655556          0.643836      0.670455\n",
      "sonar                      NaN     0.807692               NaN      0.708333\n",
      "spambase                   NaN     0.891767               NaN      0.888073\n",
      "splice                     NaN     0.837989               NaN      0.835754\n",
      "vehicle               0.890625     0.873950          0.896000      0.898305\n",
      "wdbc                       NaN     0.944882               NaN      0.919355\n",
      "wine                       NaN     1.000000               NaN      0.972973\n",
      "wine4                 0.121951     0.000000          0.121212      0.000000\n",
      "yeast3                0.671875     0.769231          0.661417      0.769231\n",
      "yeast6                0.260870     0.533333          0.260870      0.307692\n"
     ]
    }
   ],
   "source": [
    "# On  Utilise F1‑score comme métrique principale de comparaison entre les modèles linéaires => Contrairement à l’accuracy, il reste pertinent même lorsque les classes sont déséquilibrées.\n",
    "f1_table = results_linear_df.pivot_table(\n",
    "    index=\"dataset\",\n",
    "    columns=\"model_name\",\n",
    "    values=\"f1\"\n",
    ")\n",
    "print(f1_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454fcf47",
   "metadata": {},
   "source": [
    "## **4. Approches paramétriques non linéaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4eeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accuracy        f1       auc  train_time  n_train  n_test   dataset  \\\n",
      "0  0.775120  0.237838  0.557094    0.031587     2923    1254  abalone8   \n",
      "1  0.755183  0.241975  0.557864    0.059197     2923    1254  abalone8   \n",
      "2  0.852472  0.170404  0.743235    0.297291     2923    1254  abalone8   \n",
      "3  0.798246  0.321716  0.745309    0.332926     2923    1254  abalone8   \n",
      "4  0.863636  0.000000  0.740104    0.347575     2923    1254  abalone8   \n",
      "\n",
      "  model_family   model_name  imbalanced  \n",
      "0    nonlinear    tree_base        True  \n",
      "1    nonlinear  tree_adasyn        True  \n",
      "2    nonlinear      rf_base        True  \n",
      "3    nonlinear    rf_adasyn        True  \n",
      "4    nonlinear     ada_base        True  \n"
     ]
    }
   ],
   "source": [
    "# Maintenant, on va utiliser les modèles non linéaires  : Decision Tree, RandomForest, AdaBoost et GradientBoosting.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from imblearn.over_sampling import ADASYN \n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "# Decision Trees\n",
    "\n",
    "# Variante 1 : arbre de décision \"de base\"\n",
    "tree_base = DecisionTreeClassifier(\n",
    "    max_depth=None,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# Variante 2 : ADASYN + arbre de décision (pour données déséquilibrées)\n",
    "tree_adasyn = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"adasyn\", ADASYN(random_state=0)),\n",
    "        (\"tree\", DecisionTreeClassifier(\n",
    "            max_depth=None,\n",
    "            random_state=0,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Random Forests\n",
    "\n",
    "# Variante 1 : RandomForest \"standard\"\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    ") \n",
    "\n",
    "# Variante 2 : ADASYN + RandomForest\n",
    "rf_adasyn = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"adasyn\", ADASYN(random_state=0)),\n",
    "        (\"rf\", RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=None,\n",
    "            n_jobs=-1,\n",
    "            random_state=0,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ADABOOST\n",
    "\n",
    "# Variante 1 : AdaBoost \"de base\"\n",
    "ada_base = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=0,\n",
    ")  \n",
    "\n",
    "# Variante 2 : ADASYN + AdaBoost\n",
    "ada_adasyn = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"adasyn\", ADASYN(random_state=0)),\n",
    "        (\"ada\", AdaBoostClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=1.0,\n",
    "            random_state=0,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# GRADIENT BOOSTING\n",
    "\n",
    "# Variante 1 : Gradient Boosting \"standard\"\n",
    "gb_base = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=0,\n",
    ")  \n",
    "\n",
    "# Variante 2 : ADASYN + Gradient Boosting\n",
    "gb_adasyn = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"adasyn\", ADASYN(random_state=0)),\n",
    "        (\"gb\", GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=0,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Dictionnaire non linéaire : 4 modèles × 2 variantes chacun\n",
    "models_nonlinear = {\n",
    "    \"tree_base\": tree_base,\n",
    "    \"tree_adasyn\": tree_adasyn,\n",
    "    \"rf_base\": rf_base,\n",
    "    \"rf_adasyn\": rf_adasyn,\n",
    "    \"ada_base\": ada_base,\n",
    "    \"ada_adasyn\": ada_adasyn,\n",
    "    \"gb_base\": gb_base,\n",
    "    \"gb_adasyn\": gb_adasyn,\n",
    "}\n",
    "\n",
    "# Evaluation des modèles non linéaires\n",
    "results_nonlinear_rows = []\n",
    "\n",
    "for ds in datasets:\n",
    "    # On récupère les infos du dataset (notamment s'il est déséquilibré)\n",
    "    row_info = info_df[info_df[\"dataset\"] == ds].iloc[0]\n",
    "    is_imbalanced = bool(row_info[\"imbalanced\"])\n",
    "\n",
    "    # Chargement des données préparées X, y\n",
    "    X, y = data_recovery(ds)\n",
    "\n",
    "    for model_name, model in models_nonlinear.items():\n",
    "\n",
    "        # On applique les variantes \"adasyn\" uniquement aux jeux déséquilibrés.\n",
    "        # Sur les jeux équilibrés, cela n'apporte souvent rien et peut même dégrader.\n",
    "        if (\"adasyn\" in model_name) and (not is_imbalanced):\n",
    "            continue\n",
    "\n",
    "        # Évaluation selon le protocole commun (train/test stratifié, F1, AUC, etc.)\n",
    "        res = evaluate_model(model, X, y)\n",
    "\n",
    "        # On ajoute les méta-informations nécessaires pour le benchmark global\n",
    "        res.update({\n",
    "            \"dataset\": ds,\n",
    "            \"model_family\": \"nonlinear\",   \n",
    "            \"model_name\": model_name,      \n",
    "            \"imbalanced\": is_imbalanced,\n",
    "        })\n",
    "\n",
    "        results_nonlinear_rows.append(res)\n",
    "\n",
    "# DataFrame de résultats pour les méthodes non linéaires\n",
    "results_nonlinear_df = pd.DataFrame(results_nonlinear_rows)\n",
    "print(results_nonlinear_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33b6d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# je sauvgarde le résultat dans un fichier csv dans un répertoire résultats vu que pour les modèles non linéaires, ça prend plus de temps à s'exécuter\n",
    "results_nonlinear_df.to_csv(\"resultats/results_nonlinear_models.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd43ae2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name     ada_adasyn  ada_base  gb_adasyn   gb_base  rf_adasyn   rf_base  \\\n",
      "dataset                                                                         \n",
      "abalone17        0.092050  0.000000   0.080645  0.000000   0.046512  0.000000   \n",
      "abalone20        0.061856  0.000000   0.063492  0.000000   0.111111  0.000000   \n",
      "abalone8         0.344988  0.000000   0.372263  0.078431   0.321716  0.170404   \n",
      "australian            NaN  0.845714        NaN  0.828729        NaN  0.852459   \n",
      "autompg               NaN  0.853933        NaN  0.904762        NaN  0.840909   \n",
      "balance               NaN  0.982456        NaN  0.870056        NaN  0.848837   \n",
      "bankmarketing    0.511901  0.445513   0.557692  0.516228   0.500000  0.491778   \n",
      "bupa                  NaN  0.611765        NaN  0.620690        NaN  0.602740   \n",
      "german                NaN  0.554839        NaN  0.545455        NaN  0.449275   \n",
      "glass                 NaN  0.682927        NaN  0.809524        NaN  0.780488   \n",
      "hayes            0.900000  1.000000   0.900000  1.000000   0.900000  1.000000   \n",
      "heart                 NaN  0.742857        NaN  0.724638        NaN  0.742857   \n",
      "iono                  NaN  0.916667        NaN  0.904110        NaN  0.944444   \n",
      "libras           0.923077  0.833333   0.923077  0.833333   0.923077  0.444444   \n",
      "newthyroid            NaN  1.000000        NaN  0.975610        NaN  0.975610   \n",
      "pageblocks       0.745995  0.802360   0.810127  0.877907   0.861878  0.894737   \n",
      "pima                  NaN  0.643836        NaN  0.721519        NaN  0.679739   \n",
      "satimage         0.485926  0.534591   0.619433  0.599369   0.664935  0.619048   \n",
      "segmentation     0.783673  0.807882   0.894009  0.958763   0.928230  0.960000   \n",
      "sonar                 NaN  0.754717        NaN  0.750000        NaN  0.758621   \n",
      "spambase              NaN  0.913371        NaN  0.924370        NaN  0.932584   \n",
      "splice                NaN  0.951860        NaN  0.973031        NaN  0.964091   \n",
      "vehicle          0.912000  0.900000   0.933333  0.905983   0.951613  0.933333   \n",
      "wdbc                  NaN  0.945736        NaN  0.924242        NaN  0.931298   \n",
      "wine                  NaN  0.971429        NaN  0.941176        NaN  0.971429   \n",
      "wine4            0.142857  0.190476   0.111111  0.100000   0.000000  0.000000   \n",
      "yeast3           0.773109  0.804124   0.800000  0.783505   0.772277  0.725275   \n",
      "yeast6           0.450000  0.600000   0.580645  0.421053   0.538462  0.625000   \n",
      "\n",
      "model_name     tree_adasyn  tree_base  \n",
      "dataset                                \n",
      "abalone17         0.026667   0.000000  \n",
      "abalone20         0.047619   0.000000  \n",
      "abalone8          0.241975   0.237838  \n",
      "australian             NaN   0.750000  \n",
      "autompg                NaN   0.894118  \n",
      "balance                NaN   0.837989  \n",
      "bankmarketing     0.484867   0.470552  \n",
      "bupa                   NaN   0.505747  \n",
      "german                 NaN   0.474576  \n",
      "glass                  NaN   0.578947  \n",
      "hayes             0.900000   1.000000  \n",
      "heart                  NaN   0.656716  \n",
      "iono                   NaN   0.831169  \n",
      "libras            0.800000   0.444444  \n",
      "newthyroid             NaN   0.930233  \n",
      "pageblocks        0.802198   0.828402  \n",
      "pima                   NaN   0.576687  \n",
      "satimage          0.554502   0.524422  \n",
      "segmentation      0.878049   0.878788  \n",
      "sonar                  NaN   0.750000  \n",
      "spambase               NaN   0.878269  \n",
      "splice                 NaN   0.934783  \n",
      "vehicle           0.886957   0.803419  \n",
      "wdbc                   NaN   0.882353  \n",
      "wine                   NaN   0.914286  \n",
      "wine4             0.000000   0.125000  \n",
      "yeast3            0.705882   0.625000  \n",
      "yeast6            0.413793   0.434783  \n"
     ]
    }
   ],
   "source": [
    "# Construction de la table avec F1-score\n",
    "f1_table = results_nonlinear_df.pivot_table(\n",
    "    index=\"dataset\",\n",
    "    columns=\"model_name\",\n",
    "    values=\"f1\"\n",
    ")       \n",
    "print(f1_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf15ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name     ada_adasyn  ada_base  gb_adasyn   gb_base  rf_adasyn   rf_base  \\\n",
      "dataset                                                                         \n",
      "abalone17        0.826954  0.985646   0.909091  0.982456   0.967305  0.986443   \n",
      "abalone20        0.927432  0.992026   0.952951  0.990431   0.974482  0.993620   \n",
      "abalone8         0.551834  0.863636   0.725678  0.850080   0.798246  0.852472   \n",
      "australian            NaN  0.869565        NaN  0.850242        NaN  0.869565   \n",
      "autompg               NaN  0.889831        NaN  0.932203        NaN  0.881356   \n",
      "balance               NaN  0.984043        NaN  0.877660        NaN  0.861702   \n",
      "bankmarketing    0.895680  0.897965   0.905043  0.906591   0.902978  0.904306   \n",
      "bupa                  NaN  0.682692        NaN  0.682692        NaN  0.721154   \n",
      "german                NaN  0.770000        NaN  0.766667        NaN  0.746667   \n",
      "glass                 NaN  0.800000        NaN  0.876923        NaN  0.861538   \n",
      "hayes            0.950000  1.000000   0.950000  1.000000   0.950000  1.000000   \n",
      "heart                 NaN  0.777778        NaN  0.765432        NaN  0.777778   \n",
      "iono                  NaN  0.943396        NaN  0.933962        NaN  0.962264   \n",
      "libras           0.990741  0.981481   0.990741  0.981481   0.990741  0.953704   \n",
      "newthyroid            NaN  1.000000        NaN  0.984615        NaN  0.984615   \n",
      "pageblocks       0.932400  0.959196   0.954324  0.974421   0.969549  0.978076   \n",
      "pima                  NaN  0.774892        NaN  0.809524        NaN  0.787879   \n",
      "satimage         0.820300  0.923356   0.902641  0.934231   0.933195  0.941999   \n",
      "segmentation     0.923521  0.943723   0.966811  0.988456   0.978355  0.988456   \n",
      "sonar                 NaN  0.793651        NaN  0.777778        NaN  0.777778   \n",
      "spambase              NaN  0.933333        NaN  0.941304        NaN  0.947826   \n",
      "splice                NaN  0.953830        NaN  0.973767        NaN  0.965373   \n",
      "vehicle          0.956693  0.952756   0.968504  0.956693   0.976378  0.968504   \n",
      "wdbc                  NaN  0.959064        NaN  0.941520        NaN  0.947368   \n",
      "wine                  NaN  0.981481        NaN  0.962963        NaN  0.981481   \n",
      "wine4            0.800000  0.964583   0.900000  0.962500   0.945833  0.966667   \n",
      "yeast3           0.939462  0.957399   0.952915  0.952915   0.948430  0.943946   \n",
      "yeast6           0.950673  0.982063   0.970852  0.975336   0.973094  0.986547   \n",
      "\n",
      "model_name     tree_adasyn  tree_base  \n",
      "dataset                                \n",
      "abalone17         0.941786   0.976077  \n",
      "abalone20         0.968102   0.986443  \n",
      "abalone8          0.755183   0.775120  \n",
      "australian             NaN   0.787440  \n",
      "autompg                NaN   0.923729  \n",
      "balance                NaN   0.845745  \n",
      "bankmarketing     0.875774   0.874078  \n",
      "bupa                   NaN   0.586538  \n",
      "german                 NaN   0.690000  \n",
      "glass                  NaN   0.753846  \n",
      "hayes             0.950000   1.000000  \n",
      "heart                  NaN   0.716049  \n",
      "iono                   NaN   0.877358  \n",
      "libras            0.972222   0.907407  \n",
      "newthyroid             NaN   0.953846  \n",
      "pageblocks        0.956151   0.964677  \n",
      "pima                   NaN   0.701299  \n",
      "satimage          0.902641   0.904195  \n",
      "segmentation      0.963925   0.965368  \n",
      "sonar                  NaN   0.777778  \n",
      "spambase               NaN   0.902174  \n",
      "splice                 NaN   0.937041  \n",
      "vehicle           0.948819   0.909449  \n",
      "wdbc                   NaN   0.906433  \n",
      "wine                   NaN   0.944444  \n",
      "wine4             0.889583   0.941667  \n",
      "yeast3            0.932735   0.919283  \n",
      "yeast6            0.961883   0.970852  \n"
     ]
    }
   ],
   "source": [
    "# Le tableau avec accuracy, qui peut etre trompante sur des datasets déséquilibrés\n",
    "acc_table = results_nonlinear_df.pivot_table(\n",
    "    index=\"dataset\",\n",
    "    columns=\"model_name\",\n",
    "    values=\"accuracy\"\n",
    ")   \n",
    "print(acc_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
