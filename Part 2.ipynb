{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f075b7b",
   "metadata": {},
   "source": [
    "# **Part 02:  Détection de Fraudes Bancaires**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cfc00",
   "metadata": {},
   "source": [
    "## **Chargement et Exploration des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f1e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc12d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9404\\2278002492.py:4: DtypeWarning: Columns (1,2,5,6,7,8,9,15,16,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(data_path, sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "# Chargement du jeu de données \n",
    "data_path = \"datasets/data_part2.txt\"\n",
    "# J'enregistre le jeu données en format csv vu que c'est plus pratique, on sait jamais :), les colonnes sont séparées par des ;\n",
    "df=pd.read_csv(data_path, sep=\";\") \n",
    "#df.to_csv(\"datasets/fraudes_bancaires.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31110639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du jeu de données : (4646774, 23)\n",
      "Aperçu du jeu de données :\n",
      "                      ZIBZIN IDAvisAutorisationCheque FlagImpaye  \\\n",
      "0  A013010004908126703060931                 78643044          0   \n",
      "1  A013011306908024927155000                 78643045          0   \n",
      "2  A013010002908283134592527                 78643046          0   \n",
      "3  A011010002908105209831316                 78643047          0   \n",
      "4  A013010041908000125652029                 78643048          0   \n",
      "\n",
      "              Montant      DateTransaction CodeDecision VerifianceCPT1  \\\n",
      "0                  20  2017-02-01 07:32:14            1              0   \n",
      "1                  20  2017-02-01 07:43:37            1              0   \n",
      "2  57,640000000000001  2017-02-01 07:47:38            1              0   \n",
      "3  54,289999999999999  2017-02-01 07:48:48            0              1   \n",
      "4  26,899999999999999  2017-02-01 08:13:27            1              0   \n",
      "\n",
      "  VerifianceCPT2 VerifianceCPT3 D2CB  ...        TauxImpNb_RB  \\\n",
      "0              0              0  551  ...  37,186667890919111   \n",
      "1              0              0  551  ...  48,844716275908937   \n",
      "2              0              0  549  ...  73,118279569892479   \n",
      "3              1              1  267  ...  110,05692599620494   \n",
      "4              0              0  549  ...   45,36831264567185   \n",
      "\n",
      "        TauxImpNB_CPM EcartNumCheq NbrMagasin3J         DiffDateTr1  \\\n",
      "0  52,076033757361408            0            1                   4   \n",
      "1  52,076033757361408            1            2  1,7976851851851852   \n",
      "2  52,076033757361408            0            1                   4   \n",
      "3  53,554233554497365            0            1                   4   \n",
      "4  52,076033757361408            1            1  1,9971064814814814   \n",
      "\n",
      "  DiffDateTr2 DiffDateTr3          CA3TRetMtt               CA3TR  Heure  \n",
      "0           4           4                  20                   0  27134  \n",
      "1           4           4  28,609999999999999  8,6099999999999994  27817  \n",
      "2           4           4  57,640000000000001                   0  28058  \n",
      "3           4           4  54,289999999999999                   0  28128  \n",
      "4           4           4  59,149999999999999               32,25  29607  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4646774 entries, 0 to 4646773\n",
      "Data columns (total 23 columns):\n",
      " #   Column                    Dtype \n",
      "---  ------                    ----- \n",
      " 0   ZIBZIN                    object\n",
      " 1   IDAvisAutorisationCheque  object\n",
      " 2   FlagImpaye                object\n",
      " 3   Montant                   object\n",
      " 4   DateTransaction           object\n",
      " 5   CodeDecision              object\n",
      " 6   VerifianceCPT1            object\n",
      " 7   VerifianceCPT2            object\n",
      " 8   VerifianceCPT3            object\n",
      " 9   D2CB                      object\n",
      " 10  ScoringFP1                object\n",
      " 11  ScoringFP2                object\n",
      " 12  ScoringFP3                object\n",
      " 13  TauxImpNb_RB              object\n",
      " 14  TauxImpNB_CPM             object\n",
      " 15  EcartNumCheq              object\n",
      " 16  NbrMagasin3J              object\n",
      " 17  DiffDateTr1               object\n",
      " 18  DiffDateTr2               object\n",
      " 19  DiffDateTr3               object\n",
      " 20  CA3TRetMtt                object\n",
      " 21  CA3TR                     object\n",
      " 22  Heure                     object\n",
      "dtypes: object(23)\n",
      "memory usage: 815.4+ MB\n",
      "None\n",
      "Statistiques descriptives :\n",
      "                           ZIBZIN  IDAvisAutorisationCheque  FlagImpaye  \\\n",
      "count                     4646774                   4646774     4646774   \n",
      "unique                    1280127                   3831106           5   \n",
      "top     A075000041908023367242120                  80580952           0   \n",
      "freq                          217                         2     4584151   \n",
      "\n",
      "        Montant      DateTransaction  CodeDecision  VerifianceCPT1  \\\n",
      "count   4646774              4646774       4646774         4646774   \n",
      "unique    42864              3097882            10              22   \n",
      "top          30  2017-03-04 17:18:31             0               0   \n",
      "freq      37028                   14       3483828         3593113   \n",
      "\n",
      "        VerifianceCPT2  VerifianceCPT3     D2CB  ...       TauxImpNb_RB  \\\n",
      "count          4646774         4646774  4646774  ...            4646774   \n",
      "unique              34              43     1103  ...                663   \n",
      "top                  0               0        1  ...  45,36831264567185   \n",
      "freq           3593113         3593113   936673  ...             202994   \n",
      "\n",
      "             TauxImpNB_CPM EcartNumCheq NbrMagasin3J DiffDateTr1  DiffDateTr2  \\\n",
      "count              4646774      4646774      4646774     4646774      4646774   \n",
      "unique                  82        15585           10      154511        51510   \n",
      "top     52,076033757361408            0            1           4            4   \n",
      "freq                445066      4040408      4446255     3162537      3605264   \n",
      "\n",
      "        DiffDateTr3 CA3TRetMtt    CA3TR    Heure  \n",
      "count       4646774    4646774  4646774  4646774  \n",
      "unique        12665      67973    35316    71497  \n",
      "top               4         30        0    40646  \n",
      "freq        3672048      32497  4068163      207  \n",
      "\n",
      "[4 rows x 23 columns]\n",
      "Valeurs manquantes par colonne :\n",
      "ZIBZIN                      0\n",
      "IDAvisAutorisationCheque    0\n",
      "FlagImpaye                  0\n",
      "Montant                     0\n",
      "DateTransaction             0\n",
      "CodeDecision                0\n",
      "VerifianceCPT1              0\n",
      "VerifianceCPT2              0\n",
      "VerifianceCPT3              0\n",
      "D2CB                        0\n",
      "ScoringFP1                  0\n",
      "ScoringFP2                  0\n",
      "ScoringFP3                  0\n",
      "TauxImpNb_RB                0\n",
      "TauxImpNB_CPM               0\n",
      "EcartNumCheq                0\n",
      "NbrMagasin3J                0\n",
      "DiffDateTr1                 0\n",
      "DiffDateTr2                 0\n",
      "DiffDateTr3                 0\n",
      "CA3TRetMtt                  0\n",
      "CA3TR                       0\n",
      "Heure                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exploration du dataset\n",
    "\n",
    "# Dimensions  \n",
    "print(\"Dimensions du jeu de données :\", df.shape)\n",
    "# Apercu\n",
    "print(\"Aperçu du jeu de données :\")\n",
    "print(df.head())\n",
    "# Résumé des types \n",
    "print(df.info())\n",
    "# Statistiques descriptives\n",
    "print(\"Statistiques descriptives :\")\n",
    "print(df.describe())\n",
    "# Valeurs manquantes par colonne: \n",
    "print(\"Valeurs manquantes par colonne :\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108ca1b",
   "metadata": {},
   "source": [
    "## **Prétraitement des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1347008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaT dans DateTransaction : 1\n",
      "Date min : 2017-02-01 07:32:14\n",
      "Date max : 2017-11-30 22:07:13\n"
     ]
    }
   ],
   "source": [
    "# Typage de la colonne DateTransaction\n",
    "# On change la colonne de date en format datetime pour pouvoir faire le split temporel\n",
    "df[\"DateTransaction\"] = pd.to_datetime(df[\"DateTransaction\"], errors=\"coerce\")\n",
    "# Vérifier qu'on n'a pas trop de NaT\n",
    "print(\"Nombre de NaT dans DateTransaction :\", df[\"DateTransaction\"].isna().sum())\n",
    "# On vérifie que le tri chronologique est cohérent\n",
    "df = df.sort_values(\"DateTransaction\").reset_index(drop=True)\n",
    "# Contrôle des bornes temporelles\n",
    "print(\"Date min :\", df[\"DateTransaction\"].min())\n",
    "print(\"Date max :\", df[\"DateTransaction\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b235038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaT dans DateTransaction : 0\n"
     ]
    }
   ],
   "source": [
    "# Vu qu'on a une seule ligne qui ne correspond pas au format datetime, on va la supprimer\n",
    "df = df.dropna(subset=[\"DateTransaction\"]).reset_index(drop=True)\n",
    "\n",
    "# petite vérif \n",
    "print(\"Nombre de NaT dans DateTransaction :\", df[\"DateTransaction\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35bffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des colonnes quantitatives et qualitativesselon la définition dans l'énoncé du projet \n",
    "# Colonnes qualitatives : colonnes des identifiants\n",
    "col_quali = [\"ZIBZIN\", \"IDAvisAutorisationCheque\"]\n",
    "\n",
    "# Colonnes quantitatives : features + cible\n",
    "col_quanti = [\n",
    "    \"FlagImpaye\",\n",
    "    \"Montant\",\n",
    "    \"CodeDecision\",\n",
    "    \"VerifianceCPT1\",\n",
    "    \"VerifianceCPT2\",\n",
    "    \"VerifianceCPT3\",\n",
    "    \"D2CB\",\n",
    "    \"ScoringFP1\",\n",
    "    \"ScoringFP2\",\n",
    "    \"ScoringFP3\",\n",
    "    \"TauxImpNb_RB\",\n",
    "    \"TauxImpNB_CPM\",\n",
    "    \"EcartNumCheq\",\n",
    "    \"NbrMagasin3J\",\n",
    "    \"DiffDateTr1\",\n",
    "    \"DiffDateTr2\",\n",
    "    \"DiffDateTr3\",\n",
    "    \"CA3TRetMtt\",\n",
    "    \"CA3TR\",\n",
    "    \"Heure\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d67eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Types des colonnes après typage :\n",
      "ZIBZIN                              object\n",
      "IDAvisAutorisationCheque            object\n",
      "FlagImpaye                           int64\n",
      "Montant                            float64\n",
      "DateTransaction             datetime64[ns]\n",
      "CodeDecision                         int64\n",
      "VerifianceCPT1                       int64\n",
      "VerifianceCPT2                       int64\n",
      "VerifianceCPT3                       int64\n",
      "D2CB                                 int64\n",
      "ScoringFP1                         float64\n",
      "ScoringFP2                         float64\n",
      "ScoringFP3                         float64\n",
      "TauxImpNb_RB                       float64\n",
      "TauxImpNB_CPM                      float64\n",
      "EcartNumCheq                         int64\n",
      "NbrMagasin3J                         int64\n",
      "DiffDateTr1                        float64\n",
      "DiffDateTr2                        float64\n",
      "DiffDateTr3                        float64\n",
      "CA3TRetMtt                         float64\n",
      "CA3TR                              float64\n",
      "Heure                                int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Typage des colonnes selon le type \n",
    "# colonnes qualitatives en object : elles y sont déjà\n",
    "for col in col_quali:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"object\")\n",
    "\n",
    "# colonnes quantitatives \n",
    "# On commence par un nettoyage très simple  :\n",
    "#    - on remplace les virgules par des points\n",
    "#    - on convertit en numérique, valeurs invalides -> NaN\n",
    "\n",
    "for col in col_quanti:\n",
    "    if col in df.columns:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)\n",
    "            .str.replace(\",\", \".\", regex=False)\n",
    "        )\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Vérification \n",
    "print(\"\\nTypes des colonnes après typage :\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e47b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution de la variable cible FlagImpaye :\n",
      "FlagImpaye\n",
      "0    4616778\n",
      "1      29995\n",
      "Name: count, dtype: int64\n",
      "Taux de fraudes (FlagImpaye=1) : 0.65%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Maintenant on vérifie la colonne cible et on regarde sa distribution pour detecter le déséquilibre\n",
    "print(\"\\nDistribution de la variable cible FlagImpaye :\")\n",
    "print(df[\"FlagImpaye\"].value_counts())\n",
    "\n",
    "# On affiche les pourcentages\n",
    "fraud_rate = df[\"FlagImpaye\"].mean()\n",
    "# sous forme de pourcentage \n",
    "print(f\"Taux de fraudes (FlagImpaye=1) : {fraud_rate*100:.2f}%\")\n",
    "\n",
    "# On fait un petit graphique circulaire pour mieux voir  \n",
    "plt.figure(figsize=(5, 5))\n",
    "df[\"FlagImpaye\"].value_counts().plot.pie(\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=[\"lightblue\", \"salmon\"],\n",
    "    labels=[\"Non Fraude\", \"Fraude\"]\n",
    ")\n",
    "plt.ylabel(\"\")  \n",
    "plt.title(\"Répartition des Fraudes vs Non-Fraudes\")\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a3d2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille train : (3888468, 23)\n",
      "Taille test  : (737068, 23)\n"
     ]
    }
   ],
   "source": [
    "# Split temporel des données train et test \n",
    "\n",
    "# Période d'apprentissage : 2017-02-01 au 2017-08-31\n",
    "start_train = pd.to_datetime(\"2017-02-01\")\n",
    "end_train   = pd.to_datetime(\"2017-08-31\")\n",
    "\n",
    "# Période de test : 2017-09-01 au 2017-11-30\n",
    "start_test = pd.to_datetime(\"2017-09-01\")\n",
    "end_test   = pd.to_datetime(\"2017-11-30\")\n",
    "\n",
    "mask_train = (df[\"DateTransaction\"] >= start_train) & (df[\"DateTransaction\"] <= end_train)\n",
    "mask_test  = (df[\"DateTransaction\"] >= start_test) & (df[\"DateTransaction\"] <= end_test)\n",
    "\n",
    "train_df = df[mask_train].copy()\n",
    "test_df  = df[mask_test].copy()\n",
    "\n",
    "print(\"Taille train :\", train_df.shape)\n",
    "print(\"Taille test  :\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68a23c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de variables explicatives : 18\n",
      "Liste des variables utilisées :\n",
      "['Montant', 'VerifianceCPT1', 'VerifianceCPT2', 'VerifianceCPT3', 'D2CB', 'ScoringFP1', 'ScoringFP2', 'ScoringFP3', 'TauxImpNb_RB', 'TauxImpNB_CPM', 'EcartNumCheq', 'NbrMagasin3J', 'DiffDateTr1', 'DiffDateTr2', 'DiffDateTr3', 'CA3TRetMtt', 'CA3TR', 'Heure']\n"
     ]
    }
   ],
   "source": [
    "# Séparation des features et de la cible\n",
    "\n",
    "target_col = \"FlagImpaye\"\n",
    "\n",
    "# Colonnes à exclure explicitement des features: colonnes non pertiennentes \n",
    "cols_to_drop = [\n",
    "    \"FlagImpaye\",           # cible\n",
    "    \"CodeDecision\",         # info post-transaction, à ne pas utiliser en prédiction\n",
    "    \"IDAvisAutorisationCheque\",  # identifiant de transaction\n",
    "    \"ZIBZIN\",               # identifiant client/chèque\n",
    "    \"DateTransaction\"       # déjà utilisé pour le split, pas forcément utile comme feature brute, de plus ça peut poser des problèmes de fuites temporelles\n",
    "]\n",
    "\n",
    "# On garde uniquement les colonnes qui ne sont pas dans cols_to_drop\n",
    "feature_cols = [c for c in df.columns if c not in cols_to_drop]\n",
    "\n",
    "print(\"Nombre de variables explicatives :\", len(feature_cols))\n",
    "print(\"Liste des variables utilisées :\")\n",
    "print(feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d5de8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : (3888468, 18) | y_train : (3888468,)\n",
      "X_test  : (737068, 18) | y_test  : (737068,)\n"
     ]
    }
   ],
   "source": [
    "# Construction de X et y pour train et test\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[target_col].copy()\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df[target_col].copy()\n",
    "\n",
    "print(\"X_train :\", X_train.shape, \"| y_train :\", y_train.shape)\n",
    "print(\"X_test  :\", X_test.shape,  \"| y_test  :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca40661",
   "metadata": {},
   "source": [
    "## **Choix des modèles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f173ac",
   "metadata": {},
   "source": [
    "On a décidé d'utiliser plusieurs familles de modèles afin de comparer leurs performances en détection de fraude sur ce jeu de données fortement déséquilibré. Les modèles sont toujours testés en version **brute** puis avec des **méthodes de prétraitement** adaptées au déséquilibre (sur‑échantillonnage, pondération des classes, et une variante d’undersampling simple).\n",
    "\n",
    "- **Méthode non paramétrique : K‑plus proches voisins (KNN)**  \n",
    "  Utilisé seul, puis avec un sur‑échantillonnage de la classe minoritaire (SMOTE) pour améliorer la détection des fraudes.\n",
    "\n",
    "- **Méthode non linéaire : Forêt aléatoire**  \n",
    "  Utilisée en version standard, puis avec une approche cost‑sensitive via `class_weight` pour mieux tenir compte du déséquilibre de classes. Une deuxième variante avec undersampling de la classe majoritaire pour comparer l’impact de la réduction du nombre de transactions non frauduleuses.\n",
    "\n",
    "- **Méthode linéaire : Régression logistique**  \n",
    "  Baseline linéaire, testée brute puis avec `class_weight` pour pondérer davantage la classe fraude, et finalement avec une méthode de sur-échantillonnage (ADASYN).\n",
    "\n",
    "- **Méthode non supervisée : k‑means**  \n",
    "  Utilisée pour repérer des clusters de transactions atypiques, sans utiliser directement la cible pendant l’apprentissage.\n",
    "\n",
    "- **Méthode ensembliste : XGBoost**                                                 \n",
    "  Modèle de gradient boosting d’arbres de décision testé comme méthode ensembliste, avec prise en compte du déséquilibre via une pondération renforcée de la classe fraude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec14292",
   "metadata": {},
   "source": [
    "## **Entrainement et Evaluation des modèles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdeec3",
   "metadata": {},
   "source": [
    "### **Méthodes non paramétriques: KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6697ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN brut (train/test sous-échantillonnés) - F1-score (classe fraude = 1) : 0.013651877133105802\n",
      "\n",
      "Rapport de classification KNN brut :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.991     1.000     0.996     99140\n",
      "           1      0.316     0.007     0.014       860\n",
      "\n",
      "    accuracy                          0.991    100000\n",
      "   macro avg      0.654     0.503     0.505    100000\n",
      "weighted avg      0.986     0.991     0.987    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On commence par le KNN brut, notre baseline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Vu que le KNN calcule les distances entre TOUS les points, ce qui peut etre lent avec notre dataset, on va sous-échantillonner les données pour le tester\n",
    "# sous-échantillonage train \n",
    "n_train_sub = 400_000\n",
    "train_sample = train_df.sample(n=n_train_sub, random_state=42)\n",
    "\n",
    "X_train_knn = train_sample[feature_cols].copy()\n",
    "y_train_knn = train_sample[target_col].copy()\n",
    "\n",
    "# sous-échantillonage test \n",
    "n_test_sub = 100_000  \n",
    "test_sample = test_df.sample(n=n_test_sub, random_state=42)\n",
    "\n",
    "X_test_knn = test_sample[feature_cols].copy()\n",
    "y_test_knn = test_sample[target_col].copy()\n",
    "\n",
    "# on utilise des pipelines, c'est plus propre\n",
    "# On prend un KNN avec k=5 et pondération par distance\n",
    "# On standardise les données avant, c'est hyper important pour le KNN vu qu'il se base sur les distances\n",
    "knn_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5, weights=\"distance\"))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "knn_pipeline.fit(X_train_knn, y_train_knn)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_knn = knn_pipeline.predict(X_test_knn)\n",
    "\n",
    "# Évaluation\n",
    "print(\"KNN brut (train/test sous-échantillonnés) - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test_knn, y_pred_knn, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification KNN brut :\")\n",
    "print(classification_report(y_test_knn, y_pred_knn, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0293b3c",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "Le KNN brut obtient une accuracy globale très élevée (~99 %), mais cela s’explique par le fort déséquilibre du jeu de données, la quasi-totalité des transactions sont non frauduleuses, et le modèle prédit principalement la classe 0.\n",
    "\n",
    "Pour la classe non fraude, la précision et le rappel sont presque parfaits (F1 ≈ 0.996), ce qui montre que le modèle sait très bien reconnaître les transactions normales.\n",
    "\n",
    "En revanche, pour la classe fraude, les performances sont très mauvaises, le rappel est quasiment nul (~0.7 %) et le F1-score est proche de 0.01. Autrement dit, le KNN brut rate l’immense majorité des fraudes et ne constitue pas un modèle exploitable pour la détection de la classe minoritaire! ça ne vaut pas le coup a ce stade d'optimiser le K.\n",
    "\n",
    "On verra ce que ça donne avec le SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f4aac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN + SMOTE (train/test sous-échantillonnés) - F1-score (classe fraude = 1) : 0.02836085455732811\n",
      "\n",
      "Rapport de classification KNN + SMOTE :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.902     0.945     99140\n",
      "           1      0.015     0.177     0.028       860\n",
      "\n",
      "    accuracy                          0.896    100000\n",
      "   macro avg      0.504     0.539     0.487    100000\n",
      "weighted avg      0.984     0.896     0.937    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# On utilisera les memes sous-échantillons que pour le KNN brut a fin de comparer proprement \n",
    "\n",
    "# pipeline KNN + SMOTE\n",
    "knn_smote_pipeline = ImbPipeline([\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5, weights=\"distance\"))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "knn_smote_pipeline.fit(X_train_knn, y_train_knn)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_knn_smote = knn_smote_pipeline.predict(X_test_knn)\n",
    "\n",
    "# Évaluation\n",
    "print(\"KNN + SMOTE (train/test sous-échantillonnés) - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test_knn, y_pred_knn_smote, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification KNN + SMOTE :\")\n",
    "print(classification_report(y_test_knn, y_pred_knn_smote, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2545f",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "Avec SMOTE, le KNN détecte plus de fraudes (rappel fraude ≈ 17.7% au lieu de 0.7%), mais au prix d’une précision très faible (~1.5%) et d’une baisse nette de l’accuracy (~89.6%).\n",
    "\n",
    "Le F1‑score fraude ne passe que d’environ 0.014 à 0.028, ce qui reste très faible AS EXPECTED. \n",
    "\n",
    "Je pense que le KNN n’est pas adapté pour ce problème de fraude très déséquilibré.\n",
    "\n",
    "On verra ce que ça donne avec le reste des méthodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90b51e5",
   "metadata": {},
   "source": [
    "## **Méthodes non linéaires : Forêt aléatoire**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e631e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest brut - F1-score (classe fraude = 1) : 0.028151960042379295\n",
      "\n",
      "Rapport de classification RF brut :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.991     1.000     0.996    730583\n",
      "           1      0.762     0.014     0.028      6485\n",
      "\n",
      "    accuracy                          0.991    737068\n",
      "   macro avg      0.877     0.507     0.512    737068\n",
      "weighted avg      0.989     0.991     0.987    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest brut \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# On commence avec cette config pour tester, on fera du tuning après si besoin \n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Contrairement au KNN, on peut utiliser tout le dataset, les forêts aléatoires sont beauuucoup plus scalables\n",
    "# Entraînement\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_rf_base = rf_base.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"Random Forest brut - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test, y_pred_rf_base, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification RF brut :\")\n",
    "print(classification_report(y_test, y_pred_rf_base, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a432c",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "La forêt aléatoire brute obtient une excellente performance sur la classe non fraude (F1 ≈ 0.996) et une accuracy globale de ~99 %, mais elle détecte très peu de fraudes (rappel ≈ 1,4 %, F1 ≈ 0,028). Comme pour KNN brut, le modèle se concentre surtout sur la classe majoritaire et reste inadapté à la détection efficace de la classe minoritaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64857b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest balanced - F1-score (classe fraude = 1) : 0.05618333777219749\n",
      "\n",
      "Rapport de classification RF balanced :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.994     0.904     0.947    730583\n",
      "           1      0.031     0.342     0.056      6485\n",
      "\n",
      "    accuracy                          0.899    737068\n",
      "   macro avg      0.512     0.623     0.501    737068\n",
      "weighted avg      0.985     0.899     0.939    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maintenant on teste une Random Forest avec pondération des classes (cost-sensitive) pour gérer le déséquilibre\n",
    "rf_bal = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"   # pondère plus la classe fraude\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "rf_bal.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_rf_bal = rf_bal.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"Random Forest balanced - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test, y_pred_rf_bal, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification RF balanced :\")\n",
    "print(classification_report(y_test, y_pred_rf_bal, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d392e",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "C’est nettement mieux pour la classe fraude (même si ça reste faible en absolu, mais c'était attendu vu le type de problème) \n",
    "\n",
    "Avec `class_weight=\"balanced\"`, RF détecte beaucoup plus de fraudes : le rappel de la classe 1 passe d’environ 1,4 % (RF brut) à 34,2 %, et le F1-score fraude est multiplié par deux (de ~0,028 à ~0,056). En contrepartie, l’accuracy globale chute à ~89,9 % et la précision sur la classe fraude devient très faible (~3,1 %), car le modèle génère davantage de faux positifs. Cette variante cost-sensitive illustre bien le compromis classique en fraude entre meilleure détection des cas frauduleux et augmentation du nombre d’alertes à traiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee6813e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest + undersampling - F1-score (classe fraude = 1) : 0.031798293658493834\n",
      "\n",
      "Rapport de classification RF + undersampling :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.997     0.578     0.732    730583\n",
      "           1      0.016     0.783     0.032      6485\n",
      "\n",
      "    accuracy                          0.580    737068\n",
      "   macro avg      0.506     0.681     0.382    737068\n",
      "weighted avg      0.988     0.580     0.726    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testons maintenant le Random Forest avec une méthode de undersampling pour voir ce que ça donne \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rf_rus_pipeline = ImbPipeline([\n",
    "    (\"under\", RandomUnderSampler(random_state=42)),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        min_samples_leaf=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Entraînement \n",
    "rf_rus_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction \n",
    "y_pred_rf_rus = rf_rus_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "\n",
    "print(\"Random Forest + undersampling - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test, y_pred_rf_rus, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification RF + undersampling :\")\n",
    "print(classification_report(y_test, y_pred_rf_rus, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cdb59e",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "L’undersampling permet à la forêt aléatoire d’atteindre un rappel très élevé sur la classe fraude (~78 %), mais avec une précision extrêmement faible (~1,6 %) et une accuracy globale d’environ 58 %. Le modèle déclenche donc énormément de fausses alertes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac8b05",
   "metadata": {},
   "source": [
    "### **Méthodes linéaires: Régression logistique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b283e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg brute - F1-score (classe fraude = 1) : 0.004582251412860852\n",
      "\n",
      "Rapport de classification LogReg brute :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.991     1.000     0.996    730583\n",
      "           1      0.242     0.002     0.005      6485\n",
      "\n",
      "    accuracy                          0.991    737068\n",
      "   macro avg      0.617     0.501     0.500    737068\n",
      "weighted avg      0.985     0.991     0.987    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Régression logistique brute\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# On utilise la régularisation L2 Ridge pour éviter l'overfitting, avec le solver lbfgs qui est adapté aux grands datasets et on standardise comme dhab \n",
    "logreg_base = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "logreg_base.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_logreg_base = logreg_base.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"LogReg brute - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test, y_pred_logreg_base, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification LogReg brute :\")\n",
    "print(classification_report(y_test, y_pred_logreg_base, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccc4b74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg balanced - F1-score (classe fraude = 1) : 0.02984019373659884\n",
      "\n",
      "Rapport de classification LogReg balanced :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.996     0.601     0.749    730583\n",
      "           1      0.015     0.696     0.030      6485\n",
      "\n",
      "    accuracy                          0.602    737068\n",
      "   macro avg      0.505     0.649     0.390    737068\n",
      "weighted avg      0.987     0.602     0.743    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Régression logistique cost-sensitive\n",
    "\n",
    "logreg_bal = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "logreg_bal.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_logreg_bal = logreg_bal.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"LogReg balanced - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test, y_pred_logreg_bal, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification LogReg balanced :\")\n",
    "print(classification_report(y_test, y_pred_logreg_bal, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa5d61b",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "Avec la pondération des classes, la régression logistique passe d’un modèle qui ignore presque totalement les fraudes (rappel ≈ 0,2 %) à un modèle qui en détecte beaucoup plus (rappel ≈ 70 %), mais au prix d’une avalanche de faux positifs et d’une chute de l’accuracy (~60 %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca0bbb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg ADASYN - F1-score (classe fraude = 1) : 0.024233634694331062\n",
      "\n",
      "Rapport de classification LogReg ADASYN :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.996     0.431     0.601    730583\n",
      "           1      0.012     0.799     0.024      6485\n",
      "\n",
      "    accuracy                          0.434    737068\n",
      "   macro avg      0.504     0.615     0.313    737068\n",
      "weighted avg      0.987     0.434     0.596    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Essayons maintenant une régression logistique avec oversampling ADASYN vu qu'on l'a pas encore testé\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline  #  pipeline d'imblearn \n",
    "\n",
    "\n",
    "logreg_adasyn = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=42)),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "logreg_adasyn.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_logreg_adasyn = logreg_adasyn.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"LogReg ADASYN - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test, y_pred_logreg_adasyn, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification LogReg ADASYN :\")\n",
    "print(classification_report(y_test, y_pred_logreg_adasyn, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f6e93",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "Avec ADASYN, la régression logistique devient très agressive sur la détection de fraude (rappel ≈ 80 %), mais au prix d’une précision quasi nulle et d’une accuracy globale très faible (~43 %). Le modèle génère donc énormément de faux positifs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c22960",
   "metadata": {},
   "source": [
    "### **Méthodes non supervisées: K-means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edbfe6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans anomaly - F1-score (classe fraude = 1) : 0.0662557781201849\n",
      "\n",
      "Rapport de classification KMeans anomaly :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.993     0.993    730583\n",
      "           1      0.074     0.060     0.066      6485\n",
      "\n",
      "    accuracy                          0.985    737068\n",
      "   macro avg      0.533     0.527     0.529    737068\n",
      "weighted avg      0.984     0.985     0.984    737068\n",
      "\n",
      "KMeans anomaly - AUC : 0.6067427509871023\n"
     ]
    }
   ],
   "source": [
    "# On va quand meme tester le K-means comme il est beaucoup utilisé en détection de fraudes \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Standardisation \n",
    "scaler_km = StandardScaler()\n",
    "X_train_km = scaler_km.fit_transform(X_train)\n",
    "X_test_km  = scaler_km.transform(X_test)\n",
    "\n",
    "#  K-means ( 2 clusters : comportement \"normal\" vs \"atypique\")\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans.fit(X_train_km)\n",
    "\n",
    "# Score d'anomalie = distance au centroïde le plus proche (plus la distance est grande, plus c'est suspect)\n",
    "dist_train = kmeans.transform(X_train_km).min(axis=1)\n",
    "dist_test  = kmeans.transform(X_test_km).min(axis=1) \n",
    "\n",
    "# On marque comme \"fraude\" les 0.5%  des points les plus éloignés\n",
    "seuil = np.quantile(dist_train, 0.995)  \n",
    "\n",
    "y_pred_km = (dist_test > seuil).astype(int)  \n",
    "\n",
    "print(\"KMeans anomaly - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test, y_pred_km, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification KMeans anomaly :\")\n",
    "print(classification_report(y_test, y_pred_km, digits=3))\n",
    "\n",
    "# On calcule aussi la AUC en utilisant la distance comme score\n",
    "auc_km = roc_auc_score(y_test, dist_test)\n",
    "print(\"KMeans anomaly - AUC :\", auc_km)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5e884",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "K-means est utilisé ici comme méthode non supervisée : on apprend les centroïdes sur le train, puis on mesure, sur le test, la distance de chaque transaction à son centroïde le plus proche. Les points trop éloignés sont marqués comme « anomalies » (fraudes).\n",
    "\n",
    "Cette approche donne une très bonne reconnaissance des transactions normales (precision/rappel ≈ 0,99 pour la classe 0), mais une détection limitée des fraudes (F1 ≈ 0,066, AUC ≈ 0,61). Elle montre que le clustering capte un peu le comportement atypique des fraudes, mais reste nettement moins performant que les modèles supervisés (avec méthodes de prétraitement pour le déséquilibre) pour ce task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279dae87",
   "metadata": {},
   "source": [
    "## **Méthodes ensemblistes: XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90074fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:21:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\context.cc:53: Only 1 GPUs are visible, setting device ordinal to 0\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost brut - F1 fraude : 0.07570977917981073\n",
      "\n",
      "Rapport de classification XGBoost brut :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     1.000     0.996    730583\n",
      "           1      0.540     0.041     0.076      6485\n",
      "\n",
      "    accuracy                          0.991    737068\n",
      "   macro avg      0.766     0.520     0.536    737068\n",
      "weighted avg      0.988     0.991     0.988    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "xgb_brut = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    random_state=42,\n",
    "    device=\"cuda:1\"   \n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "xgb_brut.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_brut = xgb_brut.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"XGBoost brut - F1 fraude :\",\n",
    "      f1_score(y_test, y_pred_brut, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification XGBoost brut :\")\n",
    "print(classification_report(y_test, y_pred_brut, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d6d364",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "Parmi tous les modèles bruts (sans pondération, sans rééquilibrage), XGBoost est celui qui obtient le meilleur F1 sur la fraude (≈ 0,076), tout en conservant une excellente performance sur la classe normale. Cela montre que, même sans traitement du déséquilibre, une méthode ensembliste comme XGBoost exploite mieux la structure des données que les autres modèles de base, mais reste insuffisante pour une détection de fraude efficace, mais encore une fois on s'attendait à ça!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4719ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [18:13:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\context.cc:53: Only 1 GPUs are visible, setting device ordinal to 0\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost GPU - F1 fraude : 0.12975711846670707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.993     0.986     0.989    730583\n",
      "           1      0.101     0.182     0.130      6485\n",
      "\n",
      "    accuracy                          0.979    737068\n",
      "   macro avg      0.547     0.584     0.559    737068\n",
      "weighted avg      0.985     0.979     0.982    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Essayons une approche cost‑sensitive par ré‑pondération des classes avec XGBoost\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=10,\n",
    "    random_state=42,\n",
    "    # GPU\n",
    "    device=\"cuda:1\"          # pour qu'il utilise mon gpu \n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb_gpu = xgb.predict(X_test)\n",
    "\n",
    "print(\"XGBoost GPU - F1 fraude :\",\n",
    "      f1_score(y_test, y_pred_xgb_gpu, pos_label=1))\n",
    "print(classification_report(y_test, y_pred_xgb_gpu, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80422c",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "Avec ce réglage cost sensitive, XGBoost améliore nettement la détection des fraudes par rapport au modèle brut : le F1 fraude passe à ≈ 0,13 avec un rappel d’environ 18%, tout en restant très bon sur la classe normale (F1 ≈ 0,99), mais au prix d'une hausse des faux positifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ebf59",
   "metadata": {},
   "source": [
    "## **Tuning des meilleurs modèles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d3f7a",
   "metadata": {},
   "source": [
    "**Les deux meilleurs modèles sont le Random Forest et le XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454a2ba",
   "metadata": {},
   "source": [
    "### **Random Forest:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eddd5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest balanced - F1-score (classe fraude = 1) : 0.09618258388805868\n",
      "\n",
      "Rapport de classification RF balanced :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.999     0.995    730583\n",
      "           1      0.404     0.055     0.096      6485\n",
      "\n",
      "    accuracy                          0.991    737068\n",
      "   macro avg      0.698     0.527     0.546    737068\n",
      "weighted avg      0.987     0.991     0.988    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testons cette config de Random Forest avec une pénalisation plus forte des erreurs sur la classe positive (fraude) pour voir si ça améliore les résultats\n",
    "rf_tuned_1 = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1, 1:20} #On pénalise fortement les erreurs sur la classe positive\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "rf_tuned_1.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred_rf_tuned_1 = rf_tuned_1.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"Random Forest balanced tuned - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test, y_pred_rf_tuned_1, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification RF balanced tuned :\")\n",
    "print(classification_report(y_test, y_pred_rf_tuned_1, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdfd51c",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "En augmentant la pénalisation de la fraude (class_weight={0:1, 1:20}), la Random Forest détecte un peu mieux les fraudes qu’en version brute ou simplement balanced (F1 fraude ≈ 0.096 contre 0.028 et 0.056)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On essaye d'optimiser les hyperparamètres du RF avec une recherche aléatoire (RandomizedSearchCV)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "# On définit une grille de recherche pour les hyperparamètres les plus importants du RF\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 200],          \n",
    "    \"max_depth\": [10, 15, None],        \n",
    "    \"min_samples_leaf\": [1, 2, 5],      \n",
    "    \"max_features\": [\"sqrt\", \"log2\"]    \n",
    "}\n",
    "\n",
    "f1_fraud_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,      \n",
    "    scoring=f1_fraud_scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Meilleurs hyperparamètres RF (F1 fraude) :\")\n",
    "print(rf_search.best_params_)\n",
    "\n",
    "best_rf = rf_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "\n",
    "print(\"RF balanced (tuned) - F1 fraude :\",\n",
    "      f1_score(y_test, y_pred_best, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification RF balanced (tuned) :\")\n",
    "print(classification_report(y_test, y_pred_best, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e51dfc",
   "metadata": {},
   "source": [
    "**Le temps de calcul a été très long et mon PC a fini par planter :( mais ça aurait été une méthode plus efficace pour optimiser les hyperparamètres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bdd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest tuned - F1-score (classe fraude = 1) : 0.04591912309287513\n",
      "\n",
      "Rapport de classification RF tuned :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.991     1.000     0.996    730583\n",
      "           1      0.583     0.024     0.046      6485\n",
      "\n",
      "    accuracy                          0.991    737068\n",
      "   macro avg      0.787     0.512     0.521    737068\n",
      "weighted avg      0.988     0.991     0.987    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vu que la recherche aléatoire n'a pas donné de résultats, testons cette deuxième config \n",
    "rf_tuned_2 = RandomForestClassifier(\n",
    "    n_estimators=300,        # plus d’arbres pour stabiliser\n",
    "    max_depth=15,          \n",
    "    min_samples_leaf=2,     \n",
    "    max_features=\"sqrt\",    \n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0: 1, 1: 10}   \n",
    ")\n",
    "\n",
    "rf_tuned_2.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf_tuned_2 = rf_tuned_2.predict(X_test)\n",
    "\n",
    "print(\"Random Forest tuned - F1-score (classe fraude = 1) :\",\n",
    "      f1_score(y_test, y_pred_rf_tuned_2, pos_label=1))\n",
    "\n",
    "print(\"\\nRapport de classification RF tuned :\")\n",
    "print(classification_report(y_test, y_pred_rf_tuned_2, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40284cb2",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "Malgré plus d’arbres, une profondeur plus grande et une pondération modérée de la fraude (class_weight={0:1,1:10}), elle fait moins bien que la version fortement pondérée ({0:1,1:20}) pour la détection de fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19697c4c",
   "metadata": {},
   "source": [
    "### **XGBoost:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e9334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [16:05:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\context.cc:53: Only 1 GPUs are visible, setting device ordinal to 0\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur seuil F1 fraude : 0.63, F1 = 0.142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.995     0.994    730583\n",
      "           1      0.172     0.121     0.142      6485\n",
      "\n",
      "    accuracy                          0.987    737068\n",
      "   macro avg      0.582     0.558     0.568    737068\n",
      "weighted avg      0.985     0.987     0.986    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maintenant, en plus des poids pondérés, on va essayer d'optimiser le seuil de décision pour maximiser le F1-score \n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=10,\n",
    "    random_state=42,\n",
    "    device=\"cuda:1\"\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# On récupère les probabilités pour la classe 1\n",
    "y_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# On cherche le meilleur seuil pour le F1 fraude\n",
    "best_th = 0.5\n",
    "best_f1 = 0.0\n",
    "\n",
    "for th in np.linspace(0.01, 0.99, 99):\n",
    "    y_pred_th = (y_proba >= th).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_th, pos_label=1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_th = th\n",
    "\n",
    "print(f\"Meilleur seuil F1 fraude : {best_th:.2f}, F1 = {best_f1:.3f}\")\n",
    "\n",
    "# Évaluation finale avec ce seuil\n",
    "y_pred_best = (y_proba >= best_th).astype(int)\n",
    "print(classification_report(y_test, y_pred_best, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad14892",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "Avec XGBoost pondéré (scale_pos_weight=10) et un **seuil optimisé pour le F1 fraude (0.63)**, le modèle atteint un F1 fraude d’environ 0.142, nettement supérieur aux versions brutes ou uniquement pondérés. Il reste excellent sur la classe non fraude (F1 ≈ 0.994) et parvient à détecter davantage de fraudes (rappel ≈ 12%) tout en gardant une précision raisonnable (≈ 17%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a02798e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [18:40:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\context.cc:53: Only 1 GPUs are visible, setting device ordinal to 0\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur seuil F1 (config manuelle) : 0.72, F1 = 0.147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.993     0.993    730583\n",
      "           1      0.150     0.145     0.147      6485\n",
      "\n",
      "    accuracy                          0.985    737068\n",
      "   macro avg      0.571     0.569     0.570    737068\n",
      "weighted avg      0.985     0.985     0.985    737068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On essaye d'améliorer encore les résultats de XGBoost en testant une config plus agressive pour la classe positive (fraude) et en recalibrant le seuil pour maximiser le F1 sur la classe fraude\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "xgb_try = XGBClassifier(\n",
    "    n_estimators=500,      # plus d’arbres\n",
    "    max_depth=4,          # moins profonds \n",
    "    learning_rate=0.05,   # un peu plus petit\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=20,  # plus de poids à la fraude\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    "    device=\"cuda:1\"\n",
    ")\n",
    "\n",
    "xgb_try.fit(X_train, y_train)\n",
    "\n",
    "y_proba_try = xgb_try.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# recalibrage du seuil pour F1\n",
    "best_th = 0.5\n",
    "best_f1 = 0.0\n",
    "\n",
    "for th in np.linspace(0.01, 0.99, 99):\n",
    "    y_pred_th = (y_proba_try >= th).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_th, pos_label=1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_th = th\n",
    "\n",
    "print(f\"Meilleur seuil F1 (config manuelle) : {best_th:.2f}, F1 = {best_f1:.3f}\")\n",
    "\n",
    "y_pred_best = (y_proba_try >= best_th).astype(int)\n",
    "print(classification_report(y_test, y_pred_best, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ee604",
   "metadata": {},
   "source": [
    "**Interprétation:**\n",
    "\n",
    "Avec cette configuration (plus d’arbres, profondeur réduite, `learning_rate` plus faible et `scale_pos_weight=20`), XGBoost obtient son **meilleur F1 fraude** jusque‑là (≈ 0.147), tout en restant très bon sur la classe normale (F1 ≈ 0.993). Le seuil optimal plus élevé (0.72) donne un compromis où le modèle détecte davantage de fraudes qu’avec la version brute ou les premières config pondérées (rappel ≈ 14.5%) tout en gardant une précision correcte (≈ 15%), ce qui en fait ta configuration la plus convaincante pour la détection de fraude dans ce projet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recherche des meilleurs hyperparamètres pour XGBoost avec RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer, classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "xgb_base = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    "    device=\"cuda:1\"\n",
    ")\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\":    [200, 300],      \n",
    "    \"max_depth\":       [3, 4, 5],        \n",
    "    \"learning_rate\":   [0.05, 0.1],       \n",
    "    \"subsample\":       [0.8, 1.0],\n",
    "    \"colsample_bytree\":[0.8, 1.0],\n",
    "    \"scale_pos_weight\":[8, 10, 15, 20]\n",
    "}\n",
    "\n",
    "f1_fraud_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=8,              \n",
    "    scoring=f1_fraud_scorer,\n",
    "    cv=2,                  \n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Meilleurs hyperparamètres XGB:\")\n",
    "print(xgb_search.best_params_)\n",
    "\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "\n",
    "# 3) Optimisation du seuil pour le meilleur modèle\n",
    "y_proba_best = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "best_th = 0.5\n",
    "best_f1 = 0.0\n",
    "\n",
    "for th in np.linspace(0.01, 0.99, 99):\n",
    "    y_pred_th = (y_proba_best >= th).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_th, pos_label=1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_th = th\n",
    "\n",
    "print(f\"Meilleur seuil F1 fraude (best XGB) : {best_th:.2f}, F1 = {best_f1:.3f}\")\n",
    "\n",
    "y_pred_best = (y_proba_best >= best_th).astype(int)\n",
    "print(classification_report(y_test, y_pred_best, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8087cdd",
   "metadata": {},
   "source": [
    "**Pareil la recherche avec RandomizedSearchCV n'a pas aboutit en raison des ressources, mais au moins on a essayé :)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52a9eb",
   "metadata": {},
   "source": [
    "## **Synthèse des performances des modèles** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
